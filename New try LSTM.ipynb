{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,  Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import yfinance as yf\n",
    "from pandas_datareader.data import DataReader\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import quandl\n",
    "from fredapi import Fred\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_rows\",200)\n",
    "sns.set(rc={'figure.figsize':(16,10)})\n",
    "fred_key = \"df4910b2cad947d95cf6ab16ba11d74d\"\n",
    "fred = Fred(api_key = fred_key)\n",
    "quandl.ApiConfig.api_key = 'Qq5R29Xiqp2yUbb9dzNq'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(prediction, target):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 3), dpi=300)\n",
    "\n",
    "    if not isinstance(prediction, pd.DataFrame):\n",
    "        Results = pd.DataFrame(prediction, index=target.index)\n",
    "        Results.plot(ax=ax, legend=True)\n",
    "        ylim = (Results.min().min(), Results.max().max())\n",
    "    else:\n",
    "        prediction.plot(ax=ax, legend=True)\n",
    "        ylim = (0, 1)\n",
    "\n",
    "    ax.fill_between(target.index, 0, ylim[1] + 1e-2, target, facecolor='k', alpha=0.1)\n",
    "    # ax.fill_between(target.shift(-250).index, 0, ylim[1] + 1e-2, target.shift(-250), facecolor='r', alpha=0.1)\n",
    "\n",
    "    if not isinstance(prediction, pd.DataFrame):\n",
    "        legend_list = [\"Prediction\", \"NBER recession indicator\"]\n",
    "    else:\n",
    "        legend_list = [\"Prediction\", 'lower', 'upper', \"NBER recession indicator\"]\n",
    "    ax.legend(legend_list)\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "MacroCode=pd.read_csv(r'Data\\Macro Variables.csv')\n",
    "MacroCode.replace({'Average HOUST':'HOUST','S&P 500':'SP500'},inplace=True)\n",
    "\n",
    "errors=[]\n",
    "indicators={}\n",
    "freq={}\n",
    "for code in MacroCode['Variable']:\n",
    "    if not ('S&P' in code):\n",
    "        try:\n",
    "            col=fred.get_series(code).to_frame(code).squeeze()\n",
    "            if code == 'CPFF':\n",
    "                indicators[code]=(col) #.resample('MS').last()\n",
    "            elif code == \"ICSA\":\n",
    "                indicators[code]=(col) #.resample('MS').sum()\n",
    "            elif code == \"SP500\":\n",
    "                indicators[code]=(col) #.resample('MS').first()\n",
    "                # indicators['SP500']=(yf.download('^GSPC')['Close'].to_frame('SP500').squeeze())\n",
    "            else:\n",
    "                indicators[code]=(col) # .resample('M').interpolate()\n",
    "        except ValueError:\n",
    "            errors.append(code)\n",
    "indicators['S&P: indust']=(yf.download('^SP500-20')['Close'].to_frame('S&P: indust').squeeze()) # .resample(\"MS\").last()\n",
    "indicators['S&P div yield']=(quandl.get(\"MULTPL/SP500_DIV_YIELD_MONTH\").squeeze().to_frame('S&P div yield').squeeze()) # .resample(\"MS\").last()\n",
    "indicators['S&P PE ratio']=(quandl.get(\"MULTPL/SP500_PE_RATIO_MONTH\").squeeze().to_frame('S&P PE ratio').squeeze()) # .resample(\"MS\").first()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPI MS\n",
      "W875RX1 MS\n",
      "DPCERA3M086SBEA MS\n",
      "CMRMTSPL MS\n",
      "RSXFS MS\n",
      "INDPRO MS\n",
      "IPFPNSS MS\n",
      "IPFINAL MS\n",
      "IPCONGD MS\n",
      "IPDCONGD MS\n",
      "IPNCONGD MS\n",
      "IPBUSEQ MS\n",
      "IPMAT MS\n",
      "IPDMAT MS\n",
      "IPNMAT MS\n",
      "IPMANSICS MS\n",
      "IPB51222S MS\n",
      "IPFUELS MS\n",
      "CUMFNS MS\n",
      "CLF16OV MS\n",
      "CE16OV MS\n",
      "UNRATE MS\n",
      "UEMPMEAN MS\n",
      "UEMPLT5 MS\n",
      "UEMP5TO14 MS\n",
      "UEMP15OV MS\n",
      "UEMP15T26 MS\n",
      "UEMP27OV MS\n",
      "ICSA W-SAT\n",
      "PAYEMS MS\n",
      "USGOOD MS\n",
      "CES1021000001 MS\n",
      "USCONS MS\n",
      "MANEMP MS\n",
      "DMANEMP MS\n",
      "NDMANEMP MS\n",
      "SRVPRD MS\n",
      "USTPU MS\n",
      "USWTRADE MS\n",
      "USTRADE MS\n",
      "USFIRE MS\n",
      "USGOVT MS\n",
      "CES0600000007 MS\n",
      "AWOTMAN MS\n",
      "AWHMAN MS\n",
      "HOUST MS\n",
      "HOUSTNE MS\n",
      "HOUSTMW MS\n",
      "HOUSTS MS\n",
      "HOUSTW MS\n",
      "PERMIT MS\n",
      "PERMITNE MS\n",
      "PERMITMW MS\n",
      "PERMITS MS\n",
      "PERMITW MS\n",
      "ACOGNO MS\n",
      "DGORDER MS\n",
      "NEWORDER MS\n",
      "AMDMUO MS\n",
      "BUSINV MS\n",
      "ISRATIO MS\n",
      "M1SL MS\n",
      "M2SL MS\n",
      "M2REAL MS\n",
      "AMBSL MS\n",
      "TOTRESNS MS\n",
      "NONBORRES MS\n",
      "BUSLOANS MS\n",
      "REALLN MS\n",
      "NONREVSL MS\n",
      "TOTALSL MS\n",
      "SP500 B\n",
      "FEDFUNDS MS\n",
      "TB3MS MS\n",
      "TB6MS MS\n",
      "GS1 MS\n",
      "GS5 MS\n",
      "GS10 MS\n",
      "AAA MS\n",
      "BAA MS\n",
      "CPFF B\n",
      "TB3SMFFM MS\n",
      "TB6SMFFM MS\n",
      "T1YFFM MS\n",
      "T5YFFM MS\n",
      "T10YFFM MS\n",
      "AAAFFM MS\n",
      "BAAFFM MS\n",
      "TWEXMMTH MS\n",
      "EXSZUS MS\n",
      "EXJPUS MS\n",
      "EXUSUK MS\n",
      "EXCAUS MS\n",
      "WPSFD49207 MS\n",
      "WPSFD49502 MS\n",
      "WPSID61 MS\n",
      "WPSID62 MS\n",
      "WTISPLC MS\n",
      "PPICMM MS\n",
      "CPIAUCSL MS\n",
      "CPIAPPSL MS\n",
      "CPITRNSL MS\n",
      "CPIMEDSL MS\n",
      "CUSR0000SAC MS\n",
      "CUSR0000SAD MS\n",
      "CUSR0000SAS MS\n",
      "CPIULFSL MS\n",
      "CUSR0000SA0L2 MS\n",
      "CUSR0000SA0L5 MS\n",
      "PCEPI MS\n",
      "DDURRG3M086SBEA MS\n",
      "DNDGRG3M086SBEA MS\n",
      "DSERRG3M086SBEA MS\n",
      "CES0600000008 MS\n",
      "CES2000000008 MS\n",
      "CES3000000008 MS\n",
      "UMCSENT MS\n",
      "MZMSL MS\n",
      "DTCOLNVHFNM MS\n",
      "DTCTHFNM MS\n",
      "INVEST MS\n",
      "S&P: indust None\n",
      "S&P div yield None\n",
      "S&P PE ratio None\n"
     ]
    }
   ],
   "source": [
    "for key in indicators:\n",
    "    print(key,pd.infer_freq(indicators[key].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(errors)>0:\n",
    "    raise Exception(\"Erorrs found\")\n",
    "data=pd.concat(indicators,axis=1).copy() #.fillna(method='ffill') #.loc[\"1971\":].dropna(how='all').fillna(method='ffill')\n",
    "# data.to_csv('Data/HistoricalVariables.csv')\n",
    "# print(data.shape)\n",
    "# data.isna().sum().sort_values()/data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.fillna(method='ffill') #.resample('D').last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deltaX(data):\n",
    "    return data.diff()\n",
    "\n",
    "def delta2X(data):\n",
    "    return data-2*data.shift(1)+data.shift(2)\n",
    "\n",
    "def logdata(data):\n",
    "    return np.log(data)\n",
    "\n",
    "def difflog(data):\n",
    "    return np.log(data).diff()\n",
    "\n",
    "def difflog2(data):\n",
    "    return np.log(data)-2*np.log(data.shift(1))+np.log(data.shift(2))\n",
    "\n",
    "def diffpercent(data):\n",
    "    return (data/data.shift() - 1) - (data.shift()/data.shift(2) - 1)\n",
    "\n",
    "\n",
    "\n",
    "transformation={1:(lambda x: x),\n",
    "                2:deltaX,\n",
    "                3:delta2X,\n",
    "                4:logdata,\n",
    "                5:difflog,\n",
    "                6:difflog2,\n",
    "                7:diffpercent,\n",
    "                }\n",
    "\n",
    "df_transformed=pd.DataFrame()\n",
    "\n",
    "for column in data.columns:\n",
    "    type=MacroCode[MacroCode['Variable'] == column]['Transformation'].iloc[0]\n",
    "    df_transformed[column]=transformation[type](data[column])\n",
    "    df_transformed=df_transformed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RPI</th>\n",
       "      <th>W875RX1</th>\n",
       "      <th>DPCERA3M086SBEA</th>\n",
       "      <th>CMRMTSPL</th>\n",
       "      <th>RSXFS</th>\n",
       "      <th>INDPRO</th>\n",
       "      <th>IPFPNSS</th>\n",
       "      <th>IPFINAL</th>\n",
       "      <th>IPCONGD</th>\n",
       "      <th>IPDCONGD</th>\n",
       "      <th>...</th>\n",
       "      <th>CES2000000008</th>\n",
       "      <th>CES3000000008</th>\n",
       "      <th>UMCSENT</th>\n",
       "      <th>MZMSL</th>\n",
       "      <th>DTCOLNVHFNM</th>\n",
       "      <th>DTCTHFNM</th>\n",
       "      <th>INVEST</th>\n",
       "      <th>S&amp;P: indust</th>\n",
       "      <th>S&amp;P div yield</th>\n",
       "      <th>S&amp;P PE ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1971-01-01</th>\n",
       "      <td>0.008594</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.013628</td>\n",
       "      <td>0.00975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007663</td>\n",
       "      <td>0.003694</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>0.014828</td>\n",
       "      <td>0.039006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014213</td>\n",
       "      <td>0.011976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007963</td>\n",
       "      <td>-0.003878</td>\n",
       "      <td>-0.00768</td>\n",
       "      <td>0.023135</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971-01-02</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014213</td>\n",
       "      <td>-0.011976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.007963</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.00768</td>\n",
       "      <td>-0.023135</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971-01-09</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971-01-16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971-01-23</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 124 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 RPI  W875RX1  DPCERA3M086SBEA  CMRMTSPL  RSXFS    INDPRO  \\\n",
       "1971-01-01  0.008594   0.0096         0.013628   0.00975    NaN  0.007663   \n",
       "1971-01-02  0.000000   0.0000         0.000000   0.00000    NaN  0.000000   \n",
       "1971-01-09  0.000000   0.0000         0.000000   0.00000    NaN  0.000000   \n",
       "1971-01-16  0.000000   0.0000         0.000000   0.00000    NaN  0.000000   \n",
       "1971-01-23  0.000000   0.0000         0.000000   0.00000    NaN  0.000000   \n",
       "\n",
       "             IPFPNSS   IPFINAL   IPCONGD  IPDCONGD  ...  CES2000000008  \\\n",
       "1971-01-01  0.003694  0.003908  0.014828  0.039006  ...       0.014213   \n",
       "1971-01-02  0.000000  0.000000  0.000000  0.000000  ...      -0.014213   \n",
       "1971-01-09  0.000000  0.000000  0.000000  0.000000  ...       0.000000   \n",
       "1971-01-16  0.000000  0.000000  0.000000  0.000000  ...       0.000000   \n",
       "1971-01-23  0.000000  0.000000  0.000000  0.000000  ...       0.000000   \n",
       "\n",
       "            CES3000000008  UMCSENT     MZMSL  DTCOLNVHFNM  DTCTHFNM    INVEST  \\\n",
       "1971-01-01       0.011976      0.0  0.007963    -0.003878  -0.00768  0.023135   \n",
       "1971-01-02      -0.011976      0.0 -0.007963     0.003878   0.00768 -0.023135   \n",
       "1971-01-09       0.000000      0.0  0.000000     0.000000   0.00000  0.000000   \n",
       "1971-01-16       0.000000      0.0  0.000000     0.000000   0.00000  0.000000   \n",
       "1971-01-23       0.000000      0.0  0.000000     0.000000   0.00000  0.000000   \n",
       "\n",
       "            S&P: indust  S&P div yield  S&P PE ratio  \n",
       "1971-01-01          NaN            0.0      0.031962  \n",
       "1971-01-02          NaN            0.0      0.000000  \n",
       "1971-01-09          NaN            0.0      0.000000  \n",
       "1971-01-16          NaN            0.0      0.000000  \n",
       "1971-01-23          NaN            0.0      0.000000  \n",
       "\n",
       "[5 rows x 124 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df=yf.download(\"GE\")\n",
    "# df.head()\n",
    "n_lookback=30\n",
    "n_forecast=1\n",
    "\n",
    "# df=pd.read_csv('Data/Transformed HistoricalVariables.csv',index_col=0,parse_dates=True)\n",
    "df=df_transformed.loc['1971':].copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11121, 125)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RPI</th>\n",
       "      <th>W875RX1</th>\n",
       "      <th>DPCERA3M086SBEA</th>\n",
       "      <th>CMRMTSPL</th>\n",
       "      <th>RSXFS</th>\n",
       "      <th>INDPRO</th>\n",
       "      <th>IPFPNSS</th>\n",
       "      <th>IPFINAL</th>\n",
       "      <th>IPCONGD</th>\n",
       "      <th>IPDCONGD</th>\n",
       "      <th>...</th>\n",
       "      <th>CES3000000008</th>\n",
       "      <th>UMCSENT</th>\n",
       "      <th>MZMSL</th>\n",
       "      <th>DTCOLNVHFNM</th>\n",
       "      <th>DTCTHFNM</th>\n",
       "      <th>INVEST</th>\n",
       "      <th>S&amp;P: indust</th>\n",
       "      <th>S&amp;P div yield</th>\n",
       "      <th>S&amp;P PE ratio</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1971-01-01</th>\n",
       "      <td>0.008594</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.013628</td>\n",
       "      <td>0.00975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007663</td>\n",
       "      <td>0.003694</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>0.014828</td>\n",
       "      <td>0.039006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007963</td>\n",
       "      <td>-0.003878</td>\n",
       "      <td>-0.00768</td>\n",
       "      <td>0.023135</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031962</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971-01-02</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.007963</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.00768</td>\n",
       "      <td>-0.023135</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971-01-09</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971-01-16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971-01-23</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 RPI  W875RX1  DPCERA3M086SBEA  CMRMTSPL  RSXFS    INDPRO  \\\n",
       "1971-01-01  0.008594   0.0096         0.013628   0.00975    NaN  0.007663   \n",
       "1971-01-02  0.000000   0.0000         0.000000   0.00000    NaN  0.000000   \n",
       "1971-01-09  0.000000   0.0000         0.000000   0.00000    NaN  0.000000   \n",
       "1971-01-16  0.000000   0.0000         0.000000   0.00000    NaN  0.000000   \n",
       "1971-01-23  0.000000   0.0000         0.000000   0.00000    NaN  0.000000   \n",
       "\n",
       "             IPFPNSS   IPFINAL   IPCONGD  IPDCONGD  ...  CES3000000008  \\\n",
       "1971-01-01  0.003694  0.003908  0.014828  0.039006  ...       0.011976   \n",
       "1971-01-02  0.000000  0.000000  0.000000  0.000000  ...      -0.011976   \n",
       "1971-01-09  0.000000  0.000000  0.000000  0.000000  ...       0.000000   \n",
       "1971-01-16  0.000000  0.000000  0.000000  0.000000  ...       0.000000   \n",
       "1971-01-23  0.000000  0.000000  0.000000  0.000000  ...       0.000000   \n",
       "\n",
       "            UMCSENT     MZMSL  DTCOLNVHFNM  DTCTHFNM    INVEST  S&P: indust  \\\n",
       "1971-01-01      0.0  0.007963    -0.003878  -0.00768  0.023135          NaN   \n",
       "1971-01-02      0.0 -0.007963     0.003878   0.00768 -0.023135          NaN   \n",
       "1971-01-09      0.0  0.000000     0.000000   0.00000  0.000000          NaN   \n",
       "1971-01-16      0.0  0.000000     0.000000   0.00000  0.000000          NaN   \n",
       "1971-01-23      0.0  0.000000     0.000000   0.00000  0.000000          NaN   \n",
       "\n",
       "            S&P div yield  S&P PE ratio  target  \n",
       "1971-01-01            0.0      0.031962     0.0  \n",
       "1971-01-02            0.0      0.000000     0.0  \n",
       "1971-01-09            0.0      0.000000     0.0  \n",
       "1971-01-16            0.0      0.000000     0.0  \n",
       "1971-01-23            0.0      0.000000     0.0  \n",
       "\n",
       "[5 rows x 125 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "recessions =  DataReader('USRECDM', 'fred', start='1800')\n",
    "# df[f'in a recession']=recessions\n",
    "df['target'] = recessions.shift(-250)\n",
    "\n",
    "df.fillna(method='ffill',inplace=True)\n",
    "# df=df.fillna(-1)\n",
    "df=df['1971':] #.resample('MS').last()\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0.0    10099\n",
       "1.0     1022\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11121, 125)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11121,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11244\\3585764641.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mmax_vif_variable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvif\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmax_vif_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Variable\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmax_vif_variable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# Find variables highly correlated with the current variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mcorrelated_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmax_vif_variable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0mcorrelated_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorrelated_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorrelated_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mmax_vif_variable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m# Add variables to the group if their correlation exceeds a threshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\amine\\OneDrive\\Documents\\PFA\\.venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, method, min_periods, numeric_only)\u001b[0m\n\u001b[0;32m  10053\u001b[0m         \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10054\u001b[0m         \u001b[0mmat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10055\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10056\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"pearson\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10057\u001b[1;33m             \u001b[0mcorrel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibalgos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnancorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_periods\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  10058\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"spearman\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10059\u001b[0m             \u001b[0mcorrel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibalgos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnancorr_spearman\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_periods\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10060\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"kendall\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Assuming your dataframe is called 'data' and the target variable column is named 'target'\n",
    "data  # Replace ... with your data\n",
    "\n",
    "# Calculate VIF for each variable in parallel\n",
    "X = data.fillna(0) #.drop('target', axis=1)  # Drop the target variable column\n",
    "\n",
    "def calculate_vif(variable, X):\n",
    "    vif = variance_inflation_factor(X.values, X.columns.get_loc(variable))\n",
    "    return variable, vif\n",
    "\n",
    "# Number of CPU cores to utilize\n",
    "num_cores = 4  # Adjust this value as needed\n",
    "\n",
    "# Calculate VIF in parallel\n",
    "vif_results = Parallel(n_jobs=num_cores)(\n",
    "    delayed(calculate_vif)(variable, X) for variable in X.columns\n",
    ")\n",
    "\n",
    "# Create VIF dataframe\n",
    "vif = pd.DataFrame(vif_results, columns=['Variable', 'VIF'])\n",
    "\n",
    "# Sort variables by VIF score\n",
    "vif = vif.sort_values(by=\"VIF\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Group collinear variables together\n",
    "groups = []\n",
    "group_threshold = 5  # Adjust this threshold as needed\n",
    "\n",
    "while vif[\"VIF\"].max() > group_threshold:\n",
    "    max_vif_index = vif[\"VIF\"].idxmax()\n",
    "    max_vif_variable = vif.loc[max_vif_index, \"Variable\"]\n",
    "    group = [max_vif_variable]\n",
    "\n",
    "    # Find variables highly correlated with the current variable\n",
    "    correlated_vars = X.corr()[max_vif_variable].abs().sort_values(ascending=False)\n",
    "    correlated_vars = correlated_vars[correlated_vars.index != max_vif_variable]\n",
    "\n",
    "    # Add variables to the group if their correlation exceeds a threshold\n",
    "    for variable, correlation in correlated_vars.items():\n",
    "        if correlation > 0.7:  # Adjust this threshold as needed\n",
    "            group.append(variable)\n",
    "            vif = vif[vif[\"Variable\"] != variable]  # Remove the variable from further consideration\n",
    "\n",
    "    groups.append(group)\n",
    "\n",
    "# Print the groups of collinear variables\n",
    "for i, group in enumerate(groups):\n",
    "    print(f\"Group {i+1}: {group}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_training=df[:\"1999\"]\n",
    "df_for_validating=df[\"2000\":\"2003\"]\n",
    "df_for_testing=df[\"2004\":]\n",
    "print('df_for_training shape: ',df_for_training.shape)\n",
    "print('df_for_validating shape: ',df_for_validating.shape)\n",
    "print('df_for_testing shape: ',df_for_testing.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# without recessions "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading the training dataset \n",
    "# # Xtrain=df_for_training.drop([\"target\"],axis=1).fillna(0)\n",
    "# Xtrain=data.loc[df_for_training.index].fillna(0)\n",
    "# ytrain=(df_for_training['target']+1)/2\n",
    "   \n",
    "# # building the model and fitting the data\n",
    "# log_reg = sm.Logit(ytrain, Xtrain).fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createXY(dataset,n_lookback = 60,n_forecast = 3,target_col=-1):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(n_lookback, len(dataset) - n_forecast + 1):\n",
    "        X.append(dataset[i - n_lookback: i, 0:dataset.shape[1]-1])\n",
    "        Y.append(dataset[i: i + n_forecast,target_col])\n",
    "\n",
    "    return np.array(X),np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scalling\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "df_for_training_scaled = scaler.fit_transform(df_for_training.fillna(0))\n",
    "df_for_validating_scaled=scaler.transform(df_for_validating.fillna(0))\n",
    "df_for_testing_scaled=scaler.transform(df_for_testing.fillna(0))\n",
    "\n",
    "print()\n",
    "trainX,trainY=createXY(df_for_training_scaled,n_lookback=n_lookback,n_forecast=n_forecast)\n",
    "validX,validY=createXY(df_for_validating_scaled,n_lookback=n_lookback,n_forecast=n_forecast)\n",
    "testX,testY=createXY(df_for_testing_scaled,n_lookback=n_lookback,n_forecast=n_forecast)\n",
    "trainX.shape,validX.shape,testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomLoss(labels, pred,w=0.2): \n",
    "    logits=tf.math.log(pred/(1-pred))\n",
    "    return tf.nn.weighted_cross_entropy_with_logits(labels, logits, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM,  Dense, Dropout, Conv1D, MaxPool1D,Flatten,RepeatVector\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "PT_path='Model/CNN2-LSTM-final'\n",
    "f1 = EarlyStopping(monitor='loss', mode='max', verbose=1, patience=30)\n",
    "f2 = ModelCheckpoint(filepath=PT_path, monitor='val_recall', mode='max', verbose=0, save_best_only=True)\n",
    "# f3 = ModelCheckpoint(filepath=PT_path+\"/fbeta\", monitor='val_fbeta_score', mode='max', verbose=0, save_best_only=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "     Conv1D(filters=32,kernel_size=(3,),activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]),)\n",
    "    ,Conv1D(filters=32,kernel_size=(3,),activation='relu')\n",
    "    ,MaxPool1D(1)\n",
    "    ,Dropout(0.1)\n",
    "    ,Flatten()\n",
    "    ,RepeatVector(1)\n",
    "    ,LSTM(100, return_sequences=False)\n",
    "    ,Dense((n_forecast)\n",
    "           ,activation='sigmoid'\n",
    "           )\n",
    "    ])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              metrics=['Recall', 'Precision'],\n",
    "              loss=tfa.losses.SigmoidFocalCrossEntropy()\n",
    "            #   loss='BinaryCrossentropy'\n",
    "              )\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(trainX, trainY, epochs=100, \n",
    "                    batch_size=320,\n",
    "                    # validation_split=0.1,\n",
    "                    validation_data=(validX, validY),\n",
    "                    verbose=1,\n",
    "                    callbacks=[\n",
    "                        #  f1,\n",
    "                        f2,\n",
    "                        # f3\n",
    "                    ],\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have imported necessary libraries and defined the `history` object\n",
    "\n",
    "# Creating a single figure with subplots\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10,8))\n",
    "\n",
    "# Plotting Training and Validation Loss\n",
    "axes[0].plot(history.history['loss'], label='Training loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation loss')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plotting Training and Validation Recall\n",
    "axes[1].plot(history.history['recall'], label='Training Recall')\n",
    "axes[1].plot(history.history['val_recall'], label='Validation Recall')\n",
    "axes[1].set_xlabel('Epochs')\n",
    "axes[1].set_ylabel('Recall')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()  # Ensures proper spacing between subplots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model=tf.keras.models.clone_model(model)\n",
    "best_model.load_weights(PT_path)\n",
    "best_model.load_weights(\"Model\\CNN2-LSTM-110-test\")\n",
    "# best_model_f2=tf.keras.models.clone_model(model)\n",
    "# best_model_f2.load_weights(PT_path+\"/fbeta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_data(pred, origin):\n",
    "    aligned = pd.DataFrame(columns=[f'pred{i}' for i in range(n_forecast)], index=pd.date_range(\n",
    "    start=origin.index[0], freq='D', periods=pred.shape[0]+n_forecast-1))\n",
    "    for i in range(len(pred)):\n",
    "        col=i % n_forecast\n",
    "        try:\n",
    "            aligned.iloc[i:i+n_forecast,col]=pred[i]\n",
    "        except IndexError:\n",
    "            break\n",
    "    CI=aligned.std(axis=1)* 1.96 / np.sqrt(aligned.shape[1])\n",
    "    mean=aligned.mean(axis=1)\n",
    "    aligned=pd.concat([mean,mean-CI,mean+CI],keys=['m','lc',\"uc\"],axis=1)\n",
    "    return aligned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using best Recall model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.load_weights(\"Model\\CNN2-LSTM-final\")\n",
    "\n",
    "prediction_train=best_model.predict(trainX)\n",
    "prediction_valid=best_model.predict(validX)\n",
    "prediction_test=best_model.predict(testX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TT=df_for_testing.iloc[-6116:,[-1]]\n",
    "TT['Pred']=prediction_test\n",
    "TT.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_train_=model.predict(trainX)\n",
    "# min_length = min(len(df_for_training), len(prediction_train_))\n",
    "# trainResults= pd.DataFrame(index=df_for_training.iloc[-min_length:,-1].index)\n",
    "\n",
    "# trainResults['Actual']=recessions\n",
    "# trainResults['Pred']=align_data(prediction_train_, df_for_training)['m']\n",
    "# plot(trainResults['Pred'],trainResults[\"Actual\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_length = min(len(df_for_training), len(prediction_train))\n",
    "trainResults= pd.DataFrame(index=df_for_training.iloc[-min_length:,-1].index)\n",
    "\n",
    "trainResults['Actual']=recessions\n",
    "trainResults['Pred']=align_data(prediction_train, df_for_training)['m']\n",
    "\n",
    "plot(trainResults['Pred'],trainResults[\"Actual\"])\n",
    "min_length = min(len(df_for_validating), len(prediction_valid))\n",
    "validResults= pd.DataFrame(index=df_for_validating.iloc[-min_length:,-1].index)\n",
    "\n",
    "validResults['Actual']=recessions\n",
    "validResults['Pred']=align_data(prediction_valid[-min_length:], df_for_validating)['m']\n",
    "plot(validResults['Pred'],validResults['Actual'])\n",
    "min_length = min(len(df_for_testing), len(prediction_test))\n",
    "testResults= pd.DataFrame(index=df_for_testing.iloc[-min_length:,-1].index)\n",
    "\n",
    "testResults['Actual']=recessions\n",
    "testResults['Pred']=align_data(prediction_test[-min_length:], df_for_testing)['m']\n",
    "\n",
    "plot(testResults['Pred'],testResults['Actual'])\n",
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FullResults=pd.concat([trainResults,validResults,testResults]).dropna()\n",
    "FullResults['Actual']=recessions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(Results,use_recession=None):\n",
    "    Results=Results.dropna()\n",
    "    predicted = Results.Pred    \n",
    "    if use_recession is not None:\n",
    "        Results['Actual']=use_recession\n",
    "    actual = Results.Actual\n",
    "    # Convert the predicted values to binary (0 or 1)\n",
    "    predicted_binary = np.array(predicted) > 0.5\n",
    "\n",
    "    # Calculate TP, FP, FN, and TN\n",
    "    TP = np.sum((np.array(actual) == 1) & (predicted_binary == 1))\n",
    "    FP = np.sum((np.array(actual) == 0) & (predicted_binary == 1))\n",
    "    FN = np.sum((np.array(actual) == 1) & (predicted_binary == 0))\n",
    "    TN = np.sum((np.array(actual) == 0) & (predicted_binary == 0))\n",
    "\n",
    "\n",
    "    # Calculate precision, recall, and F2 score\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f2_score = 5 * precision * recall / ((4 * precision) + recall)\n",
    "    return f2_score,recall,precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testResults.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Metrics_df=pd.DataFrame(index=['train',\"validation\",'test'],columns=['f2_score','recall','precision'])\n",
    "Metrics_df.loc['train']=metrics(trainResults,recessions)\n",
    "Metrics_df.loc['validation']=metrics(validResults,recessions)\n",
    "Metrics_df.loc['test']=metrics(testResults,recessions)\n",
    "\n",
    "Metrics_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feauture importance with permutation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feauture_data=df[\"2004\":]\n",
    "\n",
    "print('Feauture_data shape: ',Feauture_data.shape)\n",
    "\n",
    "# Scalling\n",
    "\n",
    "Feauture_data_scaled=scaler.transform(Feauture_data.fillna(0))\n",
    "\n",
    "Feat_X,Feat_Y=createXY(Feauture_data_scaled,n_lookback=n_lookback,n_forecast=n_forecast)\n",
    "Feat_X.shape,Feat_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_importance(model,scaler,df_for_testing,mode='perturbation',verbose=False):\n",
    "    metric= lambda x,y: ((x-y) ** 2).mean() ** 0.5\n",
    "    Cols=df_for_testing.columns[:-1]\n",
    "    df_for_testing_scaled=scaler.transform(df_for_testing.fillna(0))\n",
    "    testX_,testY_=createXY(df_for_testing_scaled,n_lookback=n_lookback,n_forecast=n_forecast)\n",
    "    orig_out = model.predict(testX_,verbose=verbose)\n",
    "    Output=pd.DataFrame()\n",
    "    Output.loc['Base',['MSE_pred','MSE']]=metric(orig_out ,testY_)\n",
    "    for i in tqdm(range(len(Cols))):  # iterate over the three features\n",
    "        new_x = testX_.copy()\n",
    "        \n",
    "        if mode == \"perturbation\":\n",
    "            perturbation = np.random.normal(0.0, 0.2, size=new_x.shape[:2])\n",
    "            new_x[:, :, i] = new_x[:, :, i] + perturbation\n",
    "        else:\n",
    "            np.random.shuffle(new_x[:, :, i])\n",
    "            \n",
    "        perturbed_out = model.predict(new_x,verbose=verbose)\n",
    "        effect = metric(orig_out ,perturbed_out)\n",
    "        effect2 = metric(testY_ ,perturbed_out)\n",
    "        Output.loc[Cols[i],['MSE_pred','MSE']]=[effect,effect2]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Variable {Cols[i]}, perturbation effect: {effect:.4f}')\n",
    "    return Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R=var_importance(model,scaler,Feauture_data,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model with full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_training=df[:\"2003\"]\n",
    "\n",
    "df_for_testing=df[\"2004\":]\n",
    "print('df_for_training shape: ',df_for_training.shape)\n",
    "\n",
    "print('df_for_testing shape: ',df_for_testing.shape)\n",
    "\n",
    "# Scalling\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "df_for_training_scaled = scaler.fit_transform(df_for_training.fillna(0))\n",
    "df_for_testing_scaled=scaler.transform(df_for_testing.fillna(0))\n",
    "\n",
    "print()\n",
    "trainX,trainY=createXY(df_for_training_scaled,n_lookback=n_lookback,n_forecast=n_forecast)\n",
    "testX,testY=createXY(df_for_testing_scaled,n_lookback=n_lookback,n_forecast=n_forecast)\n",
    "trainX.shape,validX.shape,testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32,kernel_size=(3,),activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]),))\n",
    "model.add(Conv1D(filters=32,kernel_size=(3,),activation='relu'))\n",
    "model.add(MaxPool1D(1))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(RepeatVector(1))\n",
    "model.add(LSTM(100, return_sequences=False))\n",
    "model.add(Dense((n_forecast),activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              metrics=['Recall', 'Precision'],\n",
    "              loss=tfa.losses.SigmoidFocalCrossEntropy()\n",
    "            #   loss='BinaryCrossentropy'\n",
    "              )\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(trainX, trainY, epochs=100, \n",
    "                    batch_size=320,\n",
    "                    # validation_split=0.1,\n",
    "                    verbose=1,\n",
    "                    callbacks=[\n",
    "                        #  f1,\n",
    "                        f2,\n",
    "                        # f3\n",
    "                    ],\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have imported necessary libraries and defined the `history` object\n",
    "\n",
    "# Creating a single figure with subplots\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10,8))\n",
    "\n",
    "# Plotting Training and Validation Loss\n",
    "axes[0].plot(history.history['loss'], label='Training loss')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plotting Training and Validation Recall\n",
    "axes[1].plot(history.history['recall'], label='Training Recall')\n",
    "axes[1].set_xlabel('Epochs')\n",
    "axes[1].set_ylabel('Recall')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()  # Ensures proper spacing between subplots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_=tf.keras.models.clone_model(model)\n",
    "best_model_.load_weights(PT_path)\n",
    "prediction_train=best_model_.predict(trainX)\n",
    "prediction_valid=best_model_.predict(validX)\n",
    "prediction_test=best_model_.predict(testX)\n",
    "\n",
    "\n",
    "min_length = min(len(df_for_training), len(prediction_train))\n",
    "trainResults= pd.DataFrame(index=df_for_training.iloc[-min_length:,-1].index)\n",
    "\n",
    "trainResults['Actual']=recessions.shift(-250)\n",
    "trainResults['Pred']=align_data(prediction_train, df_for_training)['m']\n",
    "\n",
    "plot(trainResults['Pred'],trainResults[\"Actual\"])\n",
    "min_length = min(len(df_for_validating), len(prediction_valid))\n",
    "validResults= pd.DataFrame(index=df_for_validating.iloc[-min_length:,-1].index)\n",
    "\n",
    "validResults['Actual']=recessions.shift(-250)\n",
    "validResults['Pred']=align_data(prediction_valid[-min_length:], df_for_validating)['m']\n",
    "plot(validResults['Pred'],validResults['Actual'])\n",
    "min_length = min(len(df_for_testing), len(prediction_test))\n",
    "testResults= pd.DataFrame(index=df_for_testing.iloc[-min_length:,-1].index)\n",
    "\n",
    "testResults['Actual']=recessions.shift(-250)\n",
    "testResults['Pred']=align_data(prediction_test[-min_length:], df_for_testing)['m']\n",
    "\n",
    "plot(testResults['Pred'],testResults['Actual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break\n",
    "Model_path= 'Model/CNN2-LSTM-test'\n",
    "load_model=tf.keras.models.clone_model(model)\n",
    "load_model.load_weights(Model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_train=load_model.predict(trainX)\n",
    "prediction_valid=load_model.predict(validX)\n",
    "prediction_test=load_model.predict(testX)\n",
    "\n",
    "min_length = min(len(df_for_training), len(prediction_train))\n",
    "trainResults= pd.DataFrame(index=df_for_training.iloc[-min_length:,-1].index)\n",
    "\n",
    "trainResults['Actual']=recessions\n",
    "trainResults['Pred']=align_data(prediction_train, df_for_training)['m']\n",
    "\n",
    "plot(trainResults['Pred'],trainResults[\"Actual\"])\n",
    "min_length = min(len(df_for_validating), len(prediction_valid))\n",
    "validResults= pd.DataFrame(index=df_for_validating.iloc[-min_length:,-1].index)\n",
    "\n",
    "validResults['Actual']=recessions\n",
    "validResults['Pred']=align_data(prediction_valid[-min_length:], df_for_validating)['m']\n",
    "plot(validResults['Pred'],validResults['Actual'])\n",
    "min_length = min(len(df_for_testing), len(prediction_test))\n",
    "testResults= pd.DataFrame(index=df_for_testing.iloc[-min_length:,-1].index)\n",
    "\n",
    "testResults['Actual']=recessions\n",
    "testResults['Pred']=align_data(prediction_test[-min_length:], df_for_testing)['m']\n",
    "\n",
    "plot(testResults['Pred'],testResults['Actual'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tunning \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow.keras.regularizers import l1, l2,l1_l2\n",
    "\n",
    "# Define the model-building function\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=32, # hp.Int('CNN_1_filters', min_value=16, max_value=256, step=64),\n",
    "                     kernel_size=hp.Choice('CNN_1_kernel_size', values=[k for k in range(10) if k%2==1]),\n",
    "                     activation='relu',\n",
    "                     input_shape=(trainX.shape[1], trainX.shape[2]),\n",
    "                     kernel_regularizer=l1(hp.Choice('CNN1_l1_regularization', values=[0.0, 1e-3, 1e-2]))))\n",
    "    model.add(Conv1D(filters=32, # hp.Int('CNN_2_filters', min_value=16, max_value=256, step=64),\n",
    "                     kernel_size=hp.Choice('CNN_2_kernel_size', values=[k for k in range(10) if k%2==1]),\n",
    "                     activation='relu',\n",
    "                     kernel_regularizer=l2(hp.Choice('CNN1_l2_regularization', values=[0.0, 1e-3, 1e-2]))))\n",
    "    model.add(MaxPool1D(pool_size=2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(RepeatVector(n_forecast))\n",
    "    model.add(LSTM(units=100,\n",
    "                   return_sequences=hp.Boolean(\"return_sequences\",),\n",
    "                   kernel_regularizer=l1_l2(hp.Choice('LSTM_l1_l2_regularization', values=[0.0, 1e-3, 1e-2]))))\n",
    "    model.add(Dense(units=n_forecast, activation='sigmoid',\n",
    "                    kernel_regularizer=l2(hp.Choice('dense_L2_regularization', values=[0.0, 1e-3, 1e-2]))))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  metrics=['Recall', 'Precision', 'AUC', 'mse'],\n",
    "                  loss=CustomLoss)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective=kt.Objective(\"val_recall\", direction=\"max\"),\n",
    "    max_epochs=175,\n",
    "    # executions_per_trial=5,\n",
    "    factor=2,\n",
    "    overwrite=False,\n",
    "    project_name='model_tuning_withFbeta',\n",
    "    directory=\"with Custom Loss\"\n",
    ")\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "top10_model = tuner.get_best_models(num_models=10)\n",
    "\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "best_hp_model = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "# Compile the best model\n",
    "best_hp_model.compile(optimizer='adam',\n",
    "                   metrics=['Recall', 'Precision', 'AUC', 'mse'],\n",
    "                   loss=\"BinaryCrossentropy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Conv1D(filters=32,kernel_size=(3,),activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]),))\n",
    "# model.add(Conv1D(filters=32,kernel_size=(3,),activation='relu'))\n",
    "# # model.add(Conv1D(filters=32,kernel_size=(3,),activation='relu'))\n",
    "# model.add(MaxPool1D(1))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Flatten())\n",
    "# model.add(RepeatVector(1))\n",
    "# model.add(LSTM(100, return_sequences=False))\n",
    "# model.add(Dense((n_forecast)\n",
    "#             ,activation='sigmoid'\n",
    "#             ))\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]),),\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "    MaxPool1D(pool_size=2),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    RepeatVector(n_forecast),\n",
    "    LSTM(100, return_sequences=False),\n",
    "    Dense(units=n_forecast, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# fit the model\n",
    "model.compile(optimizer='adam',\n",
    "              metrics=[metric,'Recall', 'Precision', \"AUC\",],\n",
    "            #   loss=CustomLoss\n",
    "              loss='BinaryCrossentropy'\n",
    "              )\n",
    "\n",
    "model.summary()\n",
    "# fit the model\n",
    "history = model.fit(trainX, trainY, epochs=350, batch_size=32,\n",
    "                    # validation_split=0.1,\n",
    "                    validation_data=(validX, validY),\n",
    "                    verbose=1,\n",
    "                    callbacks=[\n",
    "                        #  f1,\n",
    "                        f2\n",
    "                    ],\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metric(history, metric_name, label):\n",
    "    plt.plot(history.history[metric_name], label=f'Training {label}')\n",
    "    plt.plot(history.history[f'val_{metric_name}'], label=f'Validation {label}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric_name.capitalize())\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming you have the `history` object containing the metrics history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.compile(optimizer='adam',\n",
    "                  metrics=['Recall', 'Precision', 'AUC', 'mse'],\n",
    "                  loss=CustomLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=best_model.fit(trainX, trainY, epochs=250, batch_size=320,\n",
    "                    # validation_split=0.1,\n",
    "                    validation_data=(validX, validY),\n",
    "                    verbose=1,\n",
    "                    callbacks=[\n",
    "                        #  f1,\n",
    "                        f2\n",
    "                    ],\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,model in enumerate(top10_model[n-1:n]):\n",
    "    prediction_train=model.predict(trainX)\n",
    "    prediction_valid=model.predict(validX)\n",
    "    prediction_test=model.predict(testX)\n",
    "    print(f'result for {i+1} th model ')\n",
    "    min_length = min(len(df_for_training), len(prediction_train))\n",
    "    trainResults= pd.DataFrame(index=df_for_training.iloc[-min_length:,-1].index)\n",
    "\n",
    "    trainResults['Actual']=recessions\n",
    "    trainResults['Pred']=align_data(prediction_train, df_for_training)['m']\n",
    "    plot(trainResults['Pred'],trainResults[\"Actual\"])\n",
    "    \n",
    "    min_length = min(len(df_for_validating), len(prediction_valid))\n",
    "    validResults= pd.DataFrame(index=df_for_validating.iloc[-min_length:,-1].index)\n",
    "\n",
    "    validResults['Actual']=recessions\n",
    "    validResults['Pred']=align_data(prediction_valid[-min_length:], df_for_validating)['m']\n",
    "    plot(validResults['Pred'],validResults['Actual'])\n",
    "    \n",
    "    min_length = min(len(df_for_testing), len(prediction_test))\n",
    "    testResults= pd.DataFrame(index=df_for_testing.iloc[-min_length:,-1].index)\n",
    "\n",
    "    testResults['Actual']=recessions\n",
    "    testResults['Pred']=align_data(prediction_test[-min_length:], df_for_testing)['m']\n",
    "    plot(testResults['Pred'],testResults['Actual'])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
